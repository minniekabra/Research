{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c873bbc0",
   "metadata": {},
   "source": [
    "## Exploring how word embedding of the Bert model are related - consecutive word embeddings in each layer and same word embedding of current and the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5396597d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bdlml/mkabra/mls_python_venv/miniconda/envs/mk_ve_2/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-02-24 11:36:25.006402: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-02-24 11:36:25.220668: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-02-24 11:36:26.227666: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2024-02-24 11:36:26.227766: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib64:\n",
      "2024-02-24 11:36:26.227774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22ab0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /data/bdlml/mkabra/.cache/huggingface/hub/models--bert-base-uncased/snapshots/0a6aa9128b6194f4f3c4db429b6cb4891cdb421b were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "bert_uncased=\"./models--bert-base-uncased\"\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(bert_uncased)\n",
    "model = BertModel.from_pretrained(bert_uncased)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b6a89c8",
   "metadata": {},
   "source": [
    "Adding the position of each word in the tokenized text.\n",
    "\n",
    "Tokenizing the text and decoding it then"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc6f5d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded_input torch.Size([1, 9])\n",
      "last_hidden_state shape  torch.Size([1, 9, 768])\n",
      "decoded  [[0, 101, '[CLS]'], [1, 4003, 'card'], [2, 2266, 'member'], [3, 10865, 'complained'], [4, 2055, 'about'], [5, 1996, 'the'], [6, 8169, 'banking'], [7, 2578, 'services'], [8, 102, '[SEP]']]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "text=\"card member complained about the banking services \"# express\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output1 = model(**encoded_input, output_hidden_states=True)\n",
    "print('encoded_input', encoded_input['input_ids'].shape)\n",
    "print('last_hidden_state shape ',output1['last_hidden_state'].shape)\n",
    "#print({x : tokenizer.encode(x, add_special_tokens=False) for x in text.split()})\n",
    "\n",
    "decoded=[]\n",
    "encoded=encoded_input['input_ids'][0].detach().numpy()\n",
    "for i in range(len(encoded)):\n",
    "    id_=encoded[i]\n",
    "    decoded.append([i , id_, ''.join(tokenizer.decode(id_).split(' ')) ])\n",
    "print('decoded ',decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209b4f8e",
   "metadata": {},
   "source": [
    "Comparing the similarity of two  consecutive words ('banking' & 'services') in each layer\n",
    "\n",
    "#Similarity increases with each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a58f5f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elem1  [6, 8169, 'banking']\n",
      "Elem2 [7, 2578, 'services']\n",
      "Layer no.  0 similarity b/w elements 0.144236\n",
      "Layer no.  1 similarity b/w elements 0.29189572\n",
      "Layer no.  2 similarity b/w elements 0.29617742\n",
      "Layer no.  3 similarity b/w elements 0.2963208\n",
      "Layer no.  4 similarity b/w elements 0.315475\n",
      "Layer no.  5 similarity b/w elements 0.34980845\n",
      "Layer no.  6 similarity b/w elements 0.35394484\n",
      "Layer no.  7 similarity b/w elements 0.3901196\n",
      "Layer no.  8 similarity b/w elements 0.4002793\n",
      "Layer no.  9 similarity b/w elements 0.45090446\n",
      "Layer no.  10 similarity b/w elements 0.5217607\n",
      "Layer no.  11 similarity b/w elements 0.6460583\n",
      "Layer no.  12 similarity b/w elements 0.6108783\n",
      "\n",
      "\n",
      "Elem1  [1, 4003, 'card']\n",
      "Elem2 [2, 2266, 'member']\n",
      "Layer no.  0 similarity b/w elements 0.12738381\n",
      "Layer no.  1 similarity b/w elements 0.2428741\n",
      "Layer no.  2 similarity b/w elements 0.2686418\n",
      "Layer no.  3 similarity b/w elements 0.27650452\n",
      "Layer no.  4 similarity b/w elements 0.31683552\n",
      "Layer no.  5 similarity b/w elements 0.3765126\n",
      "Layer no.  6 similarity b/w elements 0.42654282\n",
      "Layer no.  7 similarity b/w elements 0.41789636\n",
      "Layer no.  8 similarity b/w elements 0.47904003\n",
      "Layer no.  9 similarity b/w elements 0.5682531\n",
      "Layer no.  10 similarity b/w elements 0.6036862\n",
      "Layer no.  11 similarity b/w elements 0.67314667\n",
      "Layer no.  12 similarity b/w elements 0.48966008\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "cos_sim = torch.nn.CosineSimilarity(dim=0) \n",
    "\n",
    "\n",
    "def cos_sim_b_w_words_hiddenstate(elem1_pos, elem2_pos):\n",
    "    print('Elem1 ', decoded[elem1_pos] )\n",
    "    print('Elem2', decoded[elem2_pos] )\n",
    "    \n",
    "    for i in range(len(output1['hidden_states'])):\n",
    "        elem1_embed=output1['hidden_states'][i][0][elem1_pos]\n",
    "        elem2_embed=output1['hidden_states'][i][0][elem2_pos]\n",
    "        print('Layer no. ', i ,'similarity b/w elements', cos_sim(elem1_embed, elem2_embed).detach().numpy() ) \n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "cos_sim_b_w_words_hiddenstate(6,7)\n",
    "\n",
    "#Similarity increases with each layer\n",
    "#comparing the similarity of two  consecutive words ('card' & 'member')\n",
    "cos_sim_b_w_words_hiddenstate(1,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6feeb495",
   "metadata": {},
   "source": [
    "Computes cosine similarity of the 1st hidden state with the subsequent hidden states (of a word)\n",
    "\n",
    "The similarity will decrease with each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb514f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elem  [6, 8169, 'banking']\n",
      "Hidden state  1 similarity tensor(0.8465, grad_fn=<SumBackward1>)\n",
      "Hidden state  2 similarity tensor(0.7717, grad_fn=<SumBackward1>)\n",
      "Hidden state  3 similarity tensor(0.7011, grad_fn=<SumBackward1>)\n",
      "Hidden state  4 similarity tensor(0.6403, grad_fn=<SumBackward1>)\n",
      "Hidden state  5 similarity tensor(0.5595, grad_fn=<SumBackward1>)\n",
      "Hidden state  6 similarity tensor(0.5159, grad_fn=<SumBackward1>)\n",
      "Hidden state  7 similarity tensor(0.4896, grad_fn=<SumBackward1>)\n",
      "Hidden state  8 similarity tensor(0.4522, grad_fn=<SumBackward1>)\n",
      "Hidden state  9 similarity tensor(0.4168, grad_fn=<SumBackward1>)\n",
      "Hidden state  10 similarity tensor(0.3797, grad_fn=<SumBackward1>)\n",
      "Hidden state  11 similarity tensor(0.3096, grad_fn=<SumBackward1>)\n",
      "Hidden state  12 similarity tensor(0.2936, grad_fn=<SumBackward1>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def cos_sim_word_hiddenstate(elem_pos):\n",
    "    #Computes cosine similarity of the 1st hidden state with the subsequent hidden states (of a word)\n",
    "    #The similarity will decrease with each layer\n",
    "    print('Elem ', decoded[elem_pos] )\n",
    "    \n",
    "    elem_hiddenstate0_emebd=output1['hidden_states'][0][0][elem_pos]\n",
    "    \n",
    "    for i in range(1, len(output1['hidden_states'])):\n",
    "        elem_hiddenstate_N_emebd=output1['hidden_states'][i][0][elem_pos]\n",
    "        print('Hidden state ', i ,'similarity', cos_sim(elem_hiddenstate0_emebd, elem_hiddenstate_N_emebd) )\n",
    "    print('\\n')\n",
    "    \n",
    "    \n",
    "cos_sim_word_hiddenstate(6)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mk_ve_2",
   "language": "python",
   "name": "mk_ve_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
