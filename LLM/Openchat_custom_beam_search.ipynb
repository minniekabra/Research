{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gv4X98YYBw2f"
   },
   "source": [
    "# The power of constrained language models.\n",
    "\n",
    "**Why and how to build constrained language models with a custom beam search algorithm. A guide with Hugging Face code.**\n",
    "\n",
    "Pre-trained generative language models (such as OpenAI's GPT2 and GPT3) or seq2seq models (such as T5 or the recently released T0) generate free-flowing natural language. This means that their output sentences can have any shape. To get the most value out of these models, we would sometimes like the outputs to follow a certain structure. In this notebook I will show you how to achieve this and gain more value out of your language model using a custom beam search algorithm.\n",
    "\n",
    "This notebook is ment to accompany a blogpost. Read the blogpost to fully understand the benefits of a custom beam search algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNW-aiTVB9T-"
   },
   "source": [
    "## Installing the necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5Ktbg-QCN9D"
   },
   "source": [
    "## The code\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k28l1YgmOfqV"
   },
   "source": [
    "First we will download our model and corresponding tokenizer. I'm currently using GPT2 but any generative language model which has a `beam_search` method should work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "18acb2f8bd75486999a3d1f584b31726",
      "dbe1179fdd8741bfb7c23a012f182c17",
      "df34f9e086d24882b4cda07093d1235f",
      "31cffeb2ae2043dbaa0e629dfce9de4f",
      "2f7ee00674bd41bdb7a3cce6361702e2",
      "3037240ae9d04112a2d8b43ebd8cdc7e",
      "943d33be43bf442eaba045521d4f050a",
      "e9e561375b4849b5814718fc2fdcefa6",
      "3d1c7ccaabe04af4a7c58a51be899c26",
      "32c96c4467584c60b12c62ee6dfbede9",
      "9ae1abd32db94dafa62a3dac2e63d341",
      "6437371afb5c41d69ea38d70319485e8",
      "a3bfdf3e5187490697d8b84d9e7e04b5",
      "36c564b6c2a146a5abed0beeb4710f0a",
      "3b45518797234ce3abae37ed40e3037c",
      "8234327d1a4048fba312709c57756b5f",
      "dc239273fd0942c1b324fd1d88fbb129",
      "78a18f596bd746248eccf9ef8e984f23",
      "7f48726f81a74485a427130b8b5fc5b6",
      "8f5de11914be4095af9d0ad5de035f5c",
      "8e316acd7dcf4b7580c90f22ce3336d8",
      "4e0aef0eb087483c9c8611d906a2697c",
      "a4ca8f7b16f94f92b2fc5cd3fdbb4e9d",
      "a622815ece884f858ba0d54fc061debe",
      "3d5111c15c7f4b658070e69426d149b6",
      "6e31a5fb13bd4794b24421132dfcfb6a",
      "b7fbb868ea4b47b8a1dbad20fa3de76d",
      "1443638eec264317b29a9293da9fdb46",
      "ccc231efb3dd457aa4958d3c3f57d022",
      "4861d510b3854023884a74f651484049",
      "8de9cd5e25064bd28feb83bd706ccc31",
      "0192447d90114cd18ebb0cefdc2c01ee",
      "6088418aa1e74ed697cc3e6d89e8b1c3",
      "3349288ff80a4e779e28086750d56df8",
      "7632efbc6a3840c09b06278d22446e6a",
      "1d6fd75b794f4817a9fd0a8c8fb45ebf",
      "455cf69dd50843f6b1f4883cc3a4a124",
      "b38f826aac014c41ab6b91f238b6da19",
      "688bde2a0fc84365a405fb6d4fac4645",
      "d4b86245ecae45e8b7920670015dbeb0",
      "123f244dd64d47afa0543a2230102c56",
      "99f2e9166d5143e98c6f751fdb110cac",
      "42a9d745aa3c4a4bba0cf0c71487570d",
      "83285fe3851b46daaa93b5b11f373b83",
      "497feef8d4ed4642ab190efc7765e7ba",
      "fdfb4ed9aefa4d599c7c701f5efe4d24",
      "a18e33e3b3534aaa95a3b4063a6f4d51",
      "9257e352ad644905bab0005c20ef3fa0",
      "8d35dcfe91cb4281b86601dc35222c8c",
      "f7a6338b8b2b45c8917c1fb6c3a12e13",
      "adc2f65e6fe34c51bf92cbc680641f84",
      "a8c0449183b84f54a465da9462db09f9",
      "ca1078bafc8c44a1964ec8a61447b057",
      "b328e8b97e6f45bc91361e1fbec0aeff",
      "72c90e98d7654d15af192b037e168e52"
     ]
    },
    "id": "q4zpa8MjOX2_",
    "outputId": "5bd764d5-8ced-4692-8b78-88ba2aa3035f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a70efcfa7c9d4e7b976099981abcb766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/bdlml/mkabra/mls_python_venv/miniconda/envs/minnie_python3p9/lib/python3.9/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n",
      "/data/bdlml/mkabra/mls_python_venv/miniconda/envs/minnie_python3p9/lib/python3.9/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('/data2/bdlml/models/LLMs/openchat_3.5')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/data2/bdlml/models/LLMs/openchat_3.5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "import re\n",
    "from transformers import LogitsProcessor\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g5fofrauOVWS"
   },
   "source": [
    "We define a helper function: `set_scores_to_inf_for_banned_tokens`. Ignore this for now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KwJ4OLx6CRqc"
   },
   "outputs": [],
   "source": [
    "# src: https://huggingface.co/transformers/v4.1.1/_modules/transformers/generation_logits_process.html\n",
    "\n",
    "def set_scores_to_neginf_for_banned_tokens(scores, banned_tokens):\n",
    "    \"\"\"\n",
    "    Modifies the scores in place by setting the banned token positions to `-inf`. Banned token is expected to be a\n",
    "    list of list of banned tokens to ban in the format [[batch index, vocabulary position],...\n",
    "\n",
    "    Args:\n",
    "        scores: logits distribution of shape (batch size, vocabulary size)\n",
    "        banned_tokens: list of list of tokens to ban of length (batch_size)\n",
    "    \"\"\"\n",
    "    banned_mask_list = []\n",
    "    for idx, batch_banned_tokens in enumerate(banned_tokens):\n",
    "        for token in batch_banned_tokens:\n",
    "            banned_mask_list.append([idx, token])\n",
    "    if not banned_mask_list:\n",
    "        return scores\n",
    "\n",
    "    banned_mask = torch.LongTensor(banned_mask_list)\n",
    "    indices = torch.ones(len(banned_mask))\n",
    "\n",
    "    banned_mask = (\n",
    "        torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n",
    "    )\n",
    "    scores = scores.masked_fill(banned_mask, -float(\"inf\"))\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DQZ49SUVLD-A"
   },
   "source": [
    "We implement the `LogitsProcessor` class to get our desired effect. Our custom class should implement the `__call__` method of `LogitsProcessor`.\n",
    "\n",
    "This method will be called during each step of the beam search algorithm. The method takes as input the `input_ids` sequence of the partially generated beam and the `scores` of the next possible tokens.\n",
    "\n",
    "By manipulating these `scores` based on the tokens present in the `input_ids`, we can control the structure of the generated sentence.\n",
    "\n",
    "We implement two custom `LogitsProcessor` classes: `EvenLogits` and `OddLogits`. The `EvenLogits` class makes sure that all generated tokens contain an even amount of characters. The `OddLogits` do vice versa\n",
    "\n",
    "In both implementations, we achieve this by dynamically creating a list of all tokens we are not allowed to output and then setting the corresponding `scores` to `-inf` using our helper function `set_scores_to_inf_for_banned_tokens`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KMpkNhEzdDvC"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EvenLogits(LogitsProcessor):\n",
    "  def __call__(self, input_ids, scores):\n",
    "\n",
    "    banned_tokens = []\n",
    "    for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):\n",
    "      elementwise_length = np.vectorize(len)\n",
    "      keys = np.array(list(tokenizer.vocab.keys()))\n",
    "      values = np.array(list(tokenizer.vocab.values()))\n",
    "\n",
    "      # indexes of tokens that are too long\n",
    "      indexes = np.where(elementwise_length(keys) % 2 == 0)[0]\n",
    "\n",
    "      banned_tokens.append(values[indexes])\n",
    "\n",
    "    scores = set_scores_to_neginf_for_banned_tokens(scores, banned_tokens)\n",
    "    return scores\n",
    "\n",
    "class OddLogits(LogitsProcessor):\n",
    "  def __call__(self, input_ids, scores):\n",
    "\n",
    "    banned_tokens = []\n",
    "    for beam_indexx, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):\n",
    "      elementwise_length = np.vectorize(len)\n",
    "      keys = np.array(list(tokenizer.vocab.keys()))\n",
    "      values = np.array(list(tokenizer.vocab.values()))\n",
    "\n",
    "      # indexes of tokens that are too long\n",
    "      indexes = np.where(elementwise_length(keys) % 2 != 0)[0]\n",
    "\n",
    "      banned_tokens.append(values[indexes])\n",
    "\n",
    "    scores = set_scores_to_neginf_for_banned_tokens(scores, banned_tokens)\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-qzNQ4xSNeQO"
   },
   "source": [
    "We use our custom `LogitsProcessor` classes during the beam search algorithm by passing them to the `logits_processor` attribute of the `beam_search` method of our model.\n",
    "\n",
    "To devise another logits_processor, I devised 2 logits_process (on the similar lines as EvenLogits) - NumbersLogits (to predict numbers), and NoNumbersLogits (to predict not numbers), howeever, I see opposite in the results, I think I know the reason but want to confirm it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NumbersLogits(LogitsProcessor):\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "        banned_tokens = []\n",
    "        #print('input_ids ', input_ids.shape)\n",
    "        #print('scores ', scores.shape)\n",
    "        #print('output ', tokenizer.decode(input_ids[0]))\n",
    "        for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):\n",
    "            elementwise_length = np.vectorize(len)\n",
    "            keys = np.array(list(tokenizer.vocab.keys()))\n",
    "            values = np.array(list(tokenizer.vocab.values()))\n",
    "            #indexes = np.where(elementwise_length(keys) % 2 == 0)[0]\n",
    "            number_indexes=[]\n",
    "            for i, elem in enumerate(keys):\n",
    "                try:\n",
    "                    elem_int=int(elem)\n",
    "                    number_indexes.append(i)\n",
    "                except Exception as e:\n",
    "                    a=1\n",
    "            #print('number_indexes ', len(number_indexes) )#, values[number_indexes])\n",
    "            banned_tokens.append(values[number_indexes])\n",
    "        scores = set_scores_to_neginf_for_banned_tokens(scores, banned_tokens)\n",
    "        \n",
    "        for scr in scores:\n",
    "            num_occurrences=0\n",
    "            for s in scr:\n",
    "                if s==-float(\"inf\"):\n",
    "                    #print(s)\n",
    "                    num_occurrences+=1\n",
    "            #print('num_occurrences ', num_occurrences)\n",
    "        return scores\n",
    "    \n",
    "class NoNumbersLogits(LogitsProcessor):\n",
    "    \n",
    "    def __call__(self, input_ids, scores):\n",
    "        banned_tokens = []\n",
    "        #print('input_ids ', input_ids.shape)\n",
    "        #print('scores ', scores.shape)\n",
    "        #print('output ', tokenizer.decode(input_ids[0]))\n",
    "        for beam_index, (beam_input_ids, beam_scores) in enumerate(zip(input_ids, scores)):\n",
    "            elementwise_length = np.vectorize(len)\n",
    "            keys = np.array(list(tokenizer.vocab.keys()))\n",
    "            values = np.array(list(tokenizer.vocab.values()))\n",
    "            #indexes = np.where(elementwise_length(keys) % 2 == 0)[0]\n",
    "            number_indexes=[]\n",
    "            for i, elem in enumerate(keys):\n",
    "                try:\n",
    "                    elem_int=int(elem)\n",
    "                except Exception as e:\n",
    "                    a=1\n",
    "                    number_indexes.append(i)\n",
    "            #print('number_indexes ', len(number_indexes) )#, values[number_indexes])\n",
    "            banned_tokens.append(values[number_indexes])\n",
    "        scores = set_scores_to_neginf_for_banned_tokens(scores, banned_tokens)\n",
    "        \n",
    "        for scr in scores:\n",
    "            num_occurrences=0\n",
    "            for s in scr:\n",
    "                if s==-float(\"inf\"):\n",
    "                    #print(s)\n",
    "                    num_occurrences+=1\n",
    "            #print('num_occurrences ', num_occurrences)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without logits_processor  \n",
      "\n",
      "beam 0: <s> My cute doggie is a 10-year-old Chihuahua named \t even  1 odd  5\n",
      "beam 1: <s> My cute doggie is a 10-year-old Chihuahua. \t even  2 odd  3\n",
      "beam 2: <s> My cute doggie is a 10-year-old Chihuahua mix \t even  1 odd  5\n",
      "beam 3: <s> My cute doggie is a 10-year-old Chihuahua- \t even  2 odd  3\n",
      "beam 4: <s> My cute doggie is a 10-year-old Chihuahua who \t even  1 odd  5\n",
      "\n",
      "\n",
      "With OddLogits  \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2220218/395892255.py:23: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:605.)\n",
      "  torch.sparse.LongTensor(banned_mask.t(), indices, scores.size()).to(scores.device).to_dense().bool()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "beam 0: <s> My cute dog has a big personality and a big personality needs a big dog bed for comfort and \t even  1 odd  16\n",
      "beam 1: <s> My cute dog has a big personality and a big personality needs a big dog bed for a small \t even  1 odd  16\n",
      "beam 2: <s> My cute dog has a big personality and a big personality needs a big dog bed for a dog \t even  1 odd  16\n",
      "beam 3: <s> My cute dog has a big personality and a big personality needs a big dog bed for a comfortable \t even  1 odd  16\n",
      "beam 4: <s> My cute dog has a big personality and a big personality needs a big dog bed for a big \t even  1 odd  16\n",
      "\n",
      "\n",
      "With EvenLogits  \n",
      "\n",
      "beam 0: <s> My cute dog is 100% purebred American Akita. He is  \t even  9 odd  0\n",
      "beam 1: <s> My cute dog is 100% purebred American Akita. He's \t even  7 odd  0\n",
      "beam 2: <s> My cute dog is 100% purebred American Akita. He’s \t even  7 odd  0\n",
      "beam 3: <s> My cute dog is 100% purebred American Akita. He is very \t even  9 odd  0\n",
      "beam 4: <s> My cute dog is 100% purebred. He's 10 \t even  5 odd  1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 5\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 5\n",
    "max_length=20\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'My cute dog'\n",
    "\n",
    "# tokenizing the prompt\n",
    "\n",
    "print('Without logits_processor ', '\\n')\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "\n",
    "print('\\n')   \n",
    "print('With OddLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([OddLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "\n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "\n",
    "print('\\n')\n",
    "print('With EvenLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([EvenLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t','even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without logits_processor  \n",
      "\n",
      "beam 0: <s> Bollywood actor Aamir Khan is known for his perfectionism and dedication towards his \t even  2 odd  7\n",
      "beam 1: <s> Bollywood actor Aamir Khan is all set to make his digital debut with the upcoming \t even  5 odd  6\n",
      "beam 2: <s> Bollywood actor Aamir Khan is all set to return to the small screen as the \t even  6 odd  5\n",
      "beam 3: <s> Bollywood actor Aamir Khan is all set to return to the small screen with his \t even  6 odd  5\n",
      "beam 4: <s> Bollywood actor Aamir Khan is all set to return to the small screen with a \t even  6 odd  5\n",
      "With OddLogits  \n",
      "\n",
      "\n",
      "\n",
      "beam 0: <s> Bollywood actor Aamir Khan is known for his fitness and the actor never hesit \t even  1 odd  9\n",
      "beam 1: <s> Bollywood actor Aamir Khan is known for his fitness and the actor keeps sharing his \t even  1 odd  10\n",
      "beam 2: <s> Bollywood actor Aamir Khan is known for his fitness and the actor has now taken \t even  1 odd  10\n",
      "beam 3: <s> Bollywood actor Aamir Khan is known for his fitness and the actor has now started \t even  1 odd  10\n",
      "beam 4: <s> Bollywood actor Aamir Khan is known for his fitness and the actor has now decided \t even  1 odd  10\n",
      "With EvenLogits  \n",
      "\n",
      "\n",
      "\n",
      "beam 0: <s> Bollywood actor Aamir Khan is likely to be questioned by police in connection with an \t even  11 odd  0\n",
      "beam 1: <s> Bollywood actor Aamir Khan is likely to be questioned by police in connection with rape \t even  11 odd  0\n",
      "beam 2: <s> Bollywood actor Aamir Khan is likely to be questioned by police in connection with sexual \t even  11 odd  0\n",
      "beam 3: <s> Bollywood actor Aamir Khan is likely to be questioned by police in connection to an \t even  11 odd  0\n",
      "beam 4: <s> Bollywood actor Aamir Khan is likely to be seen in an upcoming film titled La \t even  11 odd  0\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 5\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 5\n",
    "\n",
    "max_length=20\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'Bollywood actor Aamir Khan is'\n",
    "\n",
    "\n",
    "# tokenizing the prompt\n",
    "\n",
    "\n",
    "print('Without logits_processor ', '\\n')\n",
    "\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "        \n",
    "\n",
    "print('With OddLogits ', '\\n')\n",
    "print('\\n')\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([OddLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=20)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "\n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "\n",
    "\n",
    "print('With EvenLogits ', '\\n')\n",
    "print('\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([EvenLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t','even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without logits_processor  \n",
      "\n",
      "beam 0: <s> Generate some adjectives which describe a good nature of a human being.\n",
      "\n",
      "1. Altruistic\n",
      "2. Benevolent\n",
      "3. Charitable\n",
      "4. Compassionate\n",
      "5. Empathetic\n",
      "6 \t even  4 odd  3\n",
      "beam 1: <s> Generate some adjectives which describe a good nature of a human being.\n",
      "\n",
      "1. Altruistic\n",
      "2. Benevolent\n",
      "3. Compassionate\n",
      "4. Empathetic\n",
      "5. Generous\n",
      "6 \t even  4 odd  3\n",
      "beam 2: <s> Generate some adjectives which describe a good nature of a human being.\n",
      "\n",
      "1. Altruistic\n",
      "2. Benevolent\n",
      "3. Caring\n",
      "4. Compassionate\n",
      "5. Empathetic\n",
      "6 \t even  4 odd  3\n",
      "beam 3: <s> Generate some adjectives which describe a good nature of a human being.\n",
      "\n",
      "1. Altruistic\n",
      "2. Benevolent\n",
      "3. Compassionate\n",
      "4. Empathetic\n",
      "5. Forgiving\n",
      " \t even  4 odd  3\n",
      "beam 4: <s> Generate some adjectives which describe a good nature of a human being.\n",
      "\n",
      "1. Altruistic\n",
      "2. Benevolent\n",
      "3. Charitable\n",
      "4. Compassionate\n",
      "5. Caring\n",
      "6. \t even  3 odd  4\n",
      "\n",
      "\n",
      "With OddLogits  \n",
      "\n",
      "beam 0: <s> Generate some adjectives which describe a good nature of a human being\n",
      "\n",
      "Adjectives\n",
      "\n",
      "Adjectives are words which provide extra information about a subject and are usually found immediately after the subject\n",
      "\n",
      "Adjectives are words which \t even  1 odd  21\n",
      "beam 1: <s> Generate some adjectives which describe a good nature of a human being\n",
      "\n",
      "Adjectives\n",
      "\n",
      "Adjectives are words which provide extra information about a subject and are usually found immediately after the subject\n",
      "\n",
      "Examples\n",
      "\n",
      "Kind\n",
      " \t even  2 odd  17\n",
      "beam 2: <s> Generate some adjectives which describe a good nature of a human being\n",
      "\n",
      "Adjectives\n",
      "\n",
      "Adjectives are words which provide extra information about a subject and are usually found immediately after the subject\n",
      "\n",
      "Examples :\n",
      "\n",
      "He \t even  1 odd  19\n",
      "beam 3: <s> Generate some adjectives which describe a good nature of a human being\n",
      "\n",
      "Adjectives\n",
      "\n",
      "Adjectives are words which provide extra information about a subject and are usually found immediately after the subject\n",
      "\n",
      "Examples\n",
      "\n",
      "Kindness \t even  1 odd  18\n",
      "beam 4: <s> Generate some adjectives which describe a good nature of a human being\n",
      "\n",
      "Adjectives\n",
      "\n",
      "Adjectives are words which provide extra information about a subject and are usually found immediately after the subject\n",
      "\n",
      "Examples\n",
      "\n",
      "Kind – \t even  1 odd  19\n",
      "\n",
      "\n",
      "With EvenLogits  \n",
      "\n",
      "beam 0: <s> Generate some adjectives which describe a good nature of a human. Good-natured, kind, generous, caring, compassionate, gentle, forgiving, friendly, warm, considerate, gentle, loving, tender, soft- \t even  3 odd  12\n",
      "beam 1: <s> Generate some adjectives which describe a good nature of a human. Good-natured, kind, generous, caring, compassionate, gentle, forgiving, friendly, warm, considerate, gentle, loving, tender, softheart \t even  3 odd  12\n",
      "beam 2: <s> Generate some adjectives which describe a good nature of a human. Good-natured, kind, generous, caring, compassionate, gentle, forgiving, friendly, warm, considerate, gentle, loving, tender, soft, \t even  3 odd  12\n",
      "beam 3: <s> Generate some adjectives which describe a good nature of a human. Good-natured, kind, generous, caring, compassionate, gentle, forgiving, friendly, warm, considerate, gentle, loving, tender, self- \t even  3 odd  12\n",
      "beam 4: <s> Generate some adjectives which describe a good nature of a human. Good-natured, kind, generous, caring, compassionate, gentle, forgiving, friendly, warm, considerate, gentle, loving, tender, empathy \t even  3 odd  12\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 5\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 5\n",
    "\n",
    "max_length=50\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'Generate some adjectives which describe a good nature of a human'\n",
    "\n",
    "# tokenizing the prompt\n",
    "\n",
    "\n",
    "print('Without logits_processor ', '\\n')\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "        \n",
    "print('\\n')\n",
    "\n",
    "print('With OddLogits ', '\\n')\n",
    "\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([OddLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "\n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('With EvenLogits ', '\\n')\n",
    "\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([EvenLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t','even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without logits_processor \n",
      "beam 0: <s> Tell me about Artificial Intelligence (AI)\n",
      "\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think like humans and mimic their actions. The term may also be applied to any machine that exhib \t even  21 odd  12\n",
      "beam 1: <s> Tell me about Artificial Intelligence (AI) and Machine Learning (ML)\n",
      "\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think like humans and mimic their actions. The term may also be \t even  21 odd  10\n",
      "beam 2: <s> Tell me about Artificial Intelligence (AI)\n",
      "\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think like humans and mimic their actions. The technology is designed to replicate human tasks such as \t even  22 odd  10\n",
      "beam 3: <s> Tell me about Artificial Intelligence (AI)\n",
      "\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think like humans and mimic their actions. The term may also be applied to any machine that demonstr \t even  22 odd  11\n",
      "beam 4: <s> Tell me about Artificial Intelligence (AI) and Machine Learning (ML)\n",
      "\n",
      "Artificial Intelligence (AI) refers to the simulation of human intelligence in machines programmed to think like humans and mimic their actions. The term “machine learning \t even  21 odd  9\n",
      "\n",
      "\n",
      "With OddLogits \n",
      "beam 0: <s> Tell me about Artificial Intelligence (AI).\n",
      "\n",
      "AI has existed for a few decades now and has evolved significantly since its early years where the primary focus was problem solving and logic based reasoning (expert systems). The field has since broadened \t even  1 odd  34\n",
      "beam 1: <s> Tell me about Artificial Intelligence (AI).\n",
      "\n",
      "AI has existed for a few decades now and has evolved significantly since its early years where the primary focus was problem solving and logic based reasoning (expert systems). The field has since grown and \t even  1 odd  35\n",
      "beam 2: <s> Tell me about Artificial Intelligence (AI).\n",
      "\n",
      "AI has existed for a few decades now and has evolved significantly since its early years where the primary focus was problem solving and logic based reasoning (expert systems).\n",
      "\n",
      "AI has since evolved \t even  1 odd  32\n",
      "beam 3: <s> Tell me about Artificial Intelligence (AI).\n",
      "\n",
      "AI has existed for a few decades now and has evolved significantly since its early years where the primary focus was problem solving and logic based reasoning (expert systems).\n",
      "\n",
      "AI has now evolved \t even  1 odd  32\n",
      "beam 4: <s> Tell me about Artificial Intelligence (AI).\n",
      "\n",
      "AI has existed for a few decades now and has evolved significantly since its early years where the primary focus was problem solving and logic based reasoning (expert systems).\n",
      "\n",
      "AI has now moved \t even  1 odd  32\n",
      "\n",
      "\n",
      "With EvenLogits \n",
      "beam 0: <s> Tell me about Artificial Intelligence. What is it? What does it do? What does it look like? What does it feel like? What is it made of? What is it made of? What is it made of? What is it \t even  28 odd  8\n",
      "beam 1: <s> Tell me about Artificial Intelligence. What is it? What does it do? What does it look like? What does it feel like? What is it made of? What is it made from? What is it made of? What is it \t even  28 odd  8\n",
      "beam 2: <s> Tell me about Artificial Intelligence. What is it? What does it do? What does it look like? What does it feel like? What is it like to interact with it? What is it like to work with it? What is it \t even  30 odd  7\n",
      "beam 3: <s> Tell me about Artificial Intelligence. What is it? What does it do? What does it look like? What does it feel like? What is it made of? What is it made of? What is it made of? What does it \t even  28 odd  8\n",
      "beam 4: <s> Tell me about Artificial Intelligence. What is it? What does it do? What does it look like? What does it feel like? What is it made of? What is it made of? What does it do? What does it do \t even  28 odd  8\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 5\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 5\n",
    "\n",
    "max_length=50\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'Tell me about Artificial Intelligence'\n",
    "\n",
    "# tokenizing the prompt\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "print('Without logits_processor ')\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "    \n",
    "print('\\n')\n",
    "\n",
    "print('With OddLogits ')\n",
    "\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([OddLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "\n",
    "  print(f'beam {index}: {output}', '\\t', 'even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "\n",
    "print('\\n')\n",
    "print('With EvenLogits ')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([EvenLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}', '\\t','even ' ,even_criteria_words.sum(), 'odd ' ,len(even_criteria_words)- even_criteria_words.sum())\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without logits_processor  \n",
      "\n",
      "beam 0: <s> Give me examples of even numbers that are not divisible by 2.\n",
      "\n",
      "Here are some examples of even numbers that are not divisible\n",
      "beam 1: <s> Give me examples of even numbers that are not divisible by 2.\n",
      "\n",
      "Even numbers are numbers that are divisible by 2.\n",
      "beam 2: <s> Give me examples of even numbers between 10 and 30.\n",
      "\n",
      "Even numbers are numbers that are divisible by 2.\n",
      "beam 3: <s> Give me examples of even numbers that are not divisible by 2.\n",
      "\n",
      "Even numbers are those that are divisible by 2.\n",
      "beam 4: <s> Give me examples of even numbers between 10 and 30.\n",
      "\n",
      "Even numbers are those that are divisible by 2.\n",
      "\n",
      "\n",
      "With NoNumbersLogits  \n",
      "\n",
      "beam 0: <s> Give me examples of even numbers12345678910111213141516\n",
      "beam 1: <s> Give me examples of even numbers12345678901234567890123\n",
      "beam 2: <s> Give me examples of even numbers12345678910121314151617\n",
      "beam 3: <s> Give me examples of even numbers12345678901112131415161\n",
      "beam 4: <s> Give me examples of even numbers12345678912345678912345\n",
      "\n",
      "\n",
      "With NumbersLogits  \n",
      "\n",
      "beam 0: <s> Give me examples of even numbers that are also prime numbers.\n",
      "\n",
      "Prime numbers are numbers that have exactly two distinct positive divisors: \n",
      "beam 1: <s> Give me examples of even numbers that are also prime numbers.\n",
      "\n",
      "Prime numbers are numbers that are only divisible by themselves and one.\n",
      "beam 2: <s> Give me examples of even numbers that are also prime numbers.\n",
      "\n",
      "Prime numbers are numbers that have exactly two distinct positive divisors: one\n",
      "beam 3: <s> Give me examples of even numbers that are also prime numbers.\n",
      "\n",
      "There are no even numbers that are also prime numbers.\n",
      "\n",
      "An even\n",
      "beam 4: <s> Give me examples of even numbers that are also prime numbers.\n",
      "\n",
      "Prime numbers are numbers that have exactly two distinct positive divisors, \n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 5\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 5\n",
    "\n",
    "max_length=30\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'Give me examples of even numbers'\n",
    "\n",
    "print('Without logits_processor ', '\\n')\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}')\n",
    "    \n",
    "print('\\n')\n",
    "print('With NoNumbersLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([NoNumbersLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "\n",
    "  print(f'beam {index}: {output}')\n",
    "\n",
    "print('\\n')\n",
    "print('With NumbersLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([NumbersLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without logits_processor  \n",
      "\n",
      "beam 0: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers(n):\n",
      "    numbers = []\n",
      "    for\n",
      "beam 1: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers():\n",
      "    numbers = []\n",
      "    for _ in\n",
      "beam 2: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers(n):\n",
      "    numbers = [random.rand\n",
      "beam 3: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_random_numbers(n):\n",
      "    numbers = []\n",
      "\n",
      "beam 4: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers():\n",
      "    numbers = []\n",
      "    for i in\n",
      "\n",
      "\n",
      "With NoNumbersLogits  \n",
      "\n",
      "beam 0: <s> Generate some numbers12345678910111213141516171\n",
      "beam 1: <s> Generate some numbers12345678901234567890123456\n",
      "beam 2: <s> Generate some numbers12345678912345678912345678\n",
      "beam 3: <s> Generate some numbers12345678909876543210987654\n",
      "beam 4: <s> Generate some numbers10000000000000000000000000\n",
      "\n",
      "\n",
      "With NumbersLogits  \n",
      "\n",
      "beam 0: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers(n):\n",
      "    numbers = []\n",
      "    for\n",
      "beam 1: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers():\n",
      "    numbers = []\n",
      "    for _ in\n",
      "beam 2: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers(n):\n",
      "    numbers = [random.rand\n",
      "beam 3: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_random_numbers(n):\n",
      "    numbers = []\n",
      "\n",
      "beam 4: <s> Generate some numbers\n",
      "\n",
      "```python\n",
      "import random\n",
      "\n",
      "def generate_numbers():\n",
      "    numbers = []\n",
      "    for i in\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 5\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 5\n",
    "\n",
    "max_length=30\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'Generate some numbers'\n",
    "\n",
    "print('Without logits_processor ', '\\n')\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}')\n",
    "    \n",
    "print('\\n')\n",
    "print('With NoNumbersLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([NoNumbersLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "\n",
    "  print(f'beam {index}: {output}')\n",
    "\n",
    "print('\\n')\n",
    "print('With NumbersLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([NumbersLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without logits_processor  \n",
      "\n",
      "beam 0: <s> Do whatever you want 100% of the time.\n",
      "\n",
      "That’s what I’ve been telling myself lately.\n",
      "\n",
      "\n",
      "beam 1: <s> Do whatever you want 100% of the time.\n",
      "\n",
      "That’s what I’ve been doing lately.\n",
      "\n",
      "I\n",
      "beam 2: <s> Do whatever you want 100% of the time.\n",
      "\n",
      "That’s what I tell myself every day.\n",
      "\n",
      "I’\n",
      "beam 3: <s> Do whatever you want 100% of the time.\n",
      "\n",
      "That’s what I’ve been doing for the last 1\n",
      "beam 4: <s> Do whatever you want 100% of the time.\n",
      "\n",
      "That’s what I’ve been doing for the past 1\n",
      "\n",
      "\n",
      "With NoNumbersLogits  \n",
      "\n",
      "beam 0: <s> Do whatever you want 100000000000000000000000\n",
      "beam 1: <s> Do whatever you want 100020002000300040005000\n",
      "beam 2: <s> Do whatever you want 100020000000000000000000\n",
      "beam 3: <s> Do whatever you want 100000000000000000000001\n",
      "beam 4: <s> Do whatever you want 100020002000000000000000\n",
      "\n",
      "\n",
      "With NumbersLogits  \n",
      "\n",
      "beam 0: <s> Do whatever you want 🤘🤘🤘🤘🤘🤘\n",
      "beam 1: <s> Do whatever you want ������������������������\n",
      "beam 2: <s> Do whatever you want 🤘🤘🤘🤘🤘\n",
      "\n",
      "I'\n",
      "beam 3: <s> Do whatever you want 🤘🤘🤘🤘🤘\n",
      "\n",
      "I’\n",
      "beam 4: <s> Do whatever you want ������������������������\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    BeamSearchScorer,\n",
    "    LogitsProcessorList,\n",
    "    StoppingCriteriaList,\n",
    "    MaxLengthCriteria\n",
    ")\n",
    "import torch\n",
    "\n",
    "# how many beams to track during the Viterbi algorithm\n",
    "num_beams = 5\n",
    "# how many beams to return after the algorithm\n",
    "num_return_beams = 5\n",
    "\n",
    "max_length=30\n",
    "\n",
    "# the prompt to continue\n",
    "prompt = 'Do whatever you want '\n",
    "\n",
    "print('Without logits_processor ', '\\n')\n",
    "\n",
    "prompt_tokenized = tokenizer(prompt, return_tensors='pt' )\n",
    "prompt_tokenized = prompt_tokenized['input_ids']\n",
    "\n",
    "\n",
    "# instantiating a BeamSearchScorer\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}')\n",
    "    \n",
    "print('\\n')\n",
    "print('With NoNumbersLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([NoNumbersLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "\n",
    "  print(f'beam {index}: {output}')\n",
    "\n",
    "print('\\n')\n",
    "print('With NumbersLogits ', '\\n')\n",
    "\n",
    "beam_scorer = BeamSearchScorer(\n",
    "    batch_size = prompt_tokenized.shape[0],\n",
    "    num_beams = num_beams,\n",
    "    num_beam_hyps_to_keep = num_return_beams,\n",
    "    device=model.device\n",
    ")\n",
    "\n",
    "logits_processor = LogitsProcessorList([NumbersLogits()])\n",
    "\n",
    "# running beam search using our custom LogitsProcessor\n",
    "generated = model.beam_search(\n",
    "    torch.cat([prompt_tokenized] * num_beams),\n",
    "    beam_scorer,\n",
    "    logits_processor = logits_processor,\n",
    "    stopping_criteria=StoppingCriteriaList([MaxLengthCriteria(max_length=max_length)])\n",
    ")\n",
    "\n",
    "# printing the output beams\n",
    "for index, output_tokenized in enumerate(generated):\n",
    "  output = tokenizer.decode(output_tokenized)\n",
    "  oup=re.sub('<s> '+ prompt, '',output )\n",
    "  elementwise_length=np.vectorize(len)\n",
    "  even_criteria_words=(elementwise_length(oup.split(' '))%2==0)\n",
    "  \n",
    "  print(f'beam {index}: {output}')\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "minnie_python3p9",
   "language": "python",
   "name": "minnie_python3p9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0192447d90114cd18ebb0cefdc2c01ee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "123f244dd64d47afa0543a2230102c56": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1443638eec264317b29a9293da9fdb46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "18acb2f8bd75486999a3d1f584b31726": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_df34f9e086d24882b4cda07093d1235f",
       "IPY_MODEL_31cffeb2ae2043dbaa0e629dfce9de4f",
       "IPY_MODEL_2f7ee00674bd41bdb7a3cce6361702e2"
      ],
      "layout": "IPY_MODEL_dbe1179fdd8741bfb7c23a012f182c17"
     }
    },
    "1d6fd75b794f4817a9fd0a8c8fb45ebf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d4b86245ecae45e8b7920670015dbeb0",
      "placeholder": "​",
      "style": "IPY_MODEL_688bde2a0fc84365a405fb6d4fac4645",
      "value": "Downloading: 100%"
     }
    },
    "2f7ee00674bd41bdb7a3cce6361702e2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ae1abd32db94dafa62a3dac2e63d341",
      "placeholder": "​",
      "style": "IPY_MODEL_32c96c4467584c60b12c62ee6dfbede9",
      "value": " 665/665 [00:00&lt;00:00, 10.0kB/s]"
     }
    },
    "3037240ae9d04112a2d8b43ebd8cdc7e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "31cffeb2ae2043dbaa0e629dfce9de4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3d1c7ccaabe04af4a7c58a51be899c26",
      "max": 665,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e9e561375b4849b5814718fc2fdcefa6",
      "value": 665
     }
    },
    "32c96c4467584c60b12c62ee6dfbede9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3349288ff80a4e779e28086750d56df8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1d6fd75b794f4817a9fd0a8c8fb45ebf",
       "IPY_MODEL_455cf69dd50843f6b1f4883cc3a4a124",
       "IPY_MODEL_b38f826aac014c41ab6b91f238b6da19"
      ],
      "layout": "IPY_MODEL_7632efbc6a3840c09b06278d22446e6a"
     }
    },
    "36c564b6c2a146a5abed0beeb4710f0a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_78a18f596bd746248eccf9ef8e984f23",
      "placeholder": "​",
      "style": "IPY_MODEL_dc239273fd0942c1b324fd1d88fbb129",
      "value": "Downloading: 100%"
     }
    },
    "3b45518797234ce3abae37ed40e3037c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8f5de11914be4095af9d0ad5de035f5c",
      "max": 1042301,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7f48726f81a74485a427130b8b5fc5b6",
      "value": 1042301
     }
    },
    "3d1c7ccaabe04af4a7c58a51be899c26": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d5111c15c7f4b658070e69426d149b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ccc231efb3dd457aa4958d3c3f57d022",
      "placeholder": "​",
      "style": "IPY_MODEL_1443638eec264317b29a9293da9fdb46",
      "value": "Downloading: 100%"
     }
    },
    "42a9d745aa3c4a4bba0cf0c71487570d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "455cf69dd50843f6b1f4883cc3a4a124": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_99f2e9166d5143e98c6f751fdb110cac",
      "max": 1355256,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_123f244dd64d47afa0543a2230102c56",
      "value": 1355256
     }
    },
    "4861d510b3854023884a74f651484049": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "497feef8d4ed4642ab190efc7765e7ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a18e33e3b3534aaa95a3b4063a6f4d51",
       "IPY_MODEL_9257e352ad644905bab0005c20ef3fa0",
       "IPY_MODEL_8d35dcfe91cb4281b86601dc35222c8c"
      ],
      "layout": "IPY_MODEL_fdfb4ed9aefa4d599c7c701f5efe4d24"
     }
    },
    "4e0aef0eb087483c9c8611d906a2697c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6088418aa1e74ed697cc3e6d89e8b1c3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6437371afb5c41d69ea38d70319485e8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_36c564b6c2a146a5abed0beeb4710f0a",
       "IPY_MODEL_3b45518797234ce3abae37ed40e3037c",
       "IPY_MODEL_8234327d1a4048fba312709c57756b5f"
      ],
      "layout": "IPY_MODEL_a3bfdf3e5187490697d8b84d9e7e04b5"
     }
    },
    "688bde2a0fc84365a405fb6d4fac4645": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6e31a5fb13bd4794b24421132dfcfb6a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8de9cd5e25064bd28feb83bd706ccc31",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4861d510b3854023884a74f651484049",
      "value": 456318
     }
    },
    "72c90e98d7654d15af192b037e168e52": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7632efbc6a3840c09b06278d22446e6a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "78a18f596bd746248eccf9ef8e984f23": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7f48726f81a74485a427130b8b5fc5b6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "8234327d1a4048fba312709c57756b5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_4e0aef0eb087483c9c8611d906a2697c",
      "placeholder": "​",
      "style": "IPY_MODEL_8e316acd7dcf4b7580c90f22ce3336d8",
      "value": " 0.99M/0.99M [00:00&lt;00:00, 10.5MB/s]"
     }
    },
    "83285fe3851b46daaa93b5b11f373b83": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8d35dcfe91cb4281b86601dc35222c8c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_72c90e98d7654d15af192b037e168e52",
      "placeholder": "​",
      "style": "IPY_MODEL_b328e8b97e6f45bc91361e1fbec0aeff",
      "value": " 523M/523M [00:13&lt;00:00, 33.8MB/s]"
     }
    },
    "8de9cd5e25064bd28feb83bd706ccc31": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8e316acd7dcf4b7580c90f22ce3336d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8f5de11914be4095af9d0ad5de035f5c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9257e352ad644905bab0005c20ef3fa0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca1078bafc8c44a1964ec8a61447b057",
      "max": 548118077,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a8c0449183b84f54a465da9462db09f9",
      "value": 548118077
     }
    },
    "943d33be43bf442eaba045521d4f050a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "99f2e9166d5143e98c6f751fdb110cac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ae1abd32db94dafa62a3dac2e63d341": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a18e33e3b3534aaa95a3b4063a6f4d51": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_adc2f65e6fe34c51bf92cbc680641f84",
      "placeholder": "​",
      "style": "IPY_MODEL_f7a6338b8b2b45c8917c1fb6c3a12e13",
      "value": "Downloading: 100%"
     }
    },
    "a3bfdf3e5187490697d8b84d9e7e04b5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4ca8f7b16f94f92b2fc5cd3fdbb4e9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3d5111c15c7f4b658070e69426d149b6",
       "IPY_MODEL_6e31a5fb13bd4794b24421132dfcfb6a",
       "IPY_MODEL_b7fbb868ea4b47b8a1dbad20fa3de76d"
      ],
      "layout": "IPY_MODEL_a622815ece884f858ba0d54fc061debe"
     }
    },
    "a622815ece884f858ba0d54fc061debe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a8c0449183b84f54a465da9462db09f9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "adc2f65e6fe34c51bf92cbc680641f84": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b328e8b97e6f45bc91361e1fbec0aeff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b38f826aac014c41ab6b91f238b6da19": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_83285fe3851b46daaa93b5b11f373b83",
      "placeholder": "​",
      "style": "IPY_MODEL_42a9d745aa3c4a4bba0cf0c71487570d",
      "value": " 1.29M/1.29M [00:00&lt;00:00, 14.8MB/s]"
     }
    },
    "b7fbb868ea4b47b8a1dbad20fa3de76d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6088418aa1e74ed697cc3e6d89e8b1c3",
      "placeholder": "​",
      "style": "IPY_MODEL_0192447d90114cd18ebb0cefdc2c01ee",
      "value": " 446k/446k [00:00&lt;00:00, 6.99MB/s]"
     }
    },
    "ca1078bafc8c44a1964ec8a61447b057": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccc231efb3dd457aa4958d3c3f57d022": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d4b86245ecae45e8b7920670015dbeb0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dbe1179fdd8741bfb7c23a012f182c17": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dc239273fd0942c1b324fd1d88fbb129": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "df34f9e086d24882b4cda07093d1235f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_943d33be43bf442eaba045521d4f050a",
      "placeholder": "​",
      "style": "IPY_MODEL_3037240ae9d04112a2d8b43ebd8cdc7e",
      "value": "Downloading: 100%"
     }
    },
    "e9e561375b4849b5814718fc2fdcefa6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f7a6338b8b2b45c8917c1fb6c3a12e13": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fdfb4ed9aefa4d599c7c701f5efe4d24": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
